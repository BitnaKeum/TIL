{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The quick brown fox jumps over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 기초\n",
    "'quick' in sentence\n",
    "sentence.index('fox')\n",
    "sentence.split().index('lazy')\n",
    "sentence.split()[2]\n",
    "sentence.split()[2][::-1] #역순으로 출력\n",
    "\n",
    "words = sentence.split()\n",
    "[words[i] for i in range(len(words)) if i%2==0]\n",
    "\n",
    "revword = [word[::-1] for word in words]\n",
    "print(' '.join(revword))  # 리스트의 각 값들을 띄어쓰기로 join하여 문장으로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoS tagging (품사 태깅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'NLP', 'Fundamentals']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"I am reading NLP Fundamentals\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\beaus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('Fundamentals', 'NNS')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 과정\n",
    "# 불용어 : 문장의 의미에 큰 영향을 미치지 않는 일반적인 단어로, 분석 시 제거한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\beaus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "print(stop_words) # 영어에서의 불용어 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'languages']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am learning Python. It is one of the most popular programming languages\"\n",
    "sentence_words = word_tokenize(sentence)\n",
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming languages\n"
     ]
    }
   ],
   "source": [
    "# 문장에서 불용어를 제거한 문장 생성\n",
    "sentence_no_stops = ' '.join([word for word in sentence_words if word not in stop_words])\n",
    "print(sentence_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정규화 과정\n",
    "# 텍스트 정규화 : 변형된 텍스트를 표준 형식으로 변환하는 것 (ex: does = doing = do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I visited US from UK on 22-10-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited United States from United Kingdom on 22-10-2018\n"
     ]
    }
   ],
   "source": [
    "normalized_sentence = sentence.replace(\"US\", \"United States\").replace(\"UK\", \"United Kingdom\").replace(\"-18\", \"-2018\")\n",
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 철자 수정\n",
    "from autocorrect import Speller\n",
    "spell = Speller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell('Natureal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ntural', 'Luanguage', 'Processin', 'deals', 'with', 'the', 'art', 'of', 'extracting', 'insightes', 'from', 'Natural', 'Languaes']\n"
     ]
    }
   ],
   "source": [
    "sentence = word_tokenize(\"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing deals with the art of extracting insights from Natural Languages\n"
     ]
    }
   ],
   "source": [
    "sentence_corrected = ' '.join([spell(word) for word in sentence])\n",
    "print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어간 추출 (ex: production -> product, products -> product)\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer.stem(\"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"coming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battl'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"battling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\beaus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출 : 어간 추출 과정에서 부적절한 결과를 극복하기 위해, 추가적으로 사전을 통해 단어의 기본 형태를 추출하는 과정\n",
    "# 시간이 오래 걸림\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battling'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('battling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\beaus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\beaus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개체명 취급\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"We are reading a book published by Packt which is based out of Birmingham.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NE', [('Packt', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)), binary=True)\n",
    "[a for a in i if len(a)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 중의성 해결\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Keep your savings in the bank\"\n",
    "sentence2 = \"It's so risky to drive over the banks of the road\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n"
     ]
    }
   ],
   "source": [
    "print(lesk(word_tokenize(sentence1), 'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.v.07')\n"
     ]
    }
   ],
   "source": [
    "print(lesk(word_tokenize(sentence2), 'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 경계 인식\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N.L.P is interesting.', \"Isn't it?\"]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"N.L.P is interesting. Isn't it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습1: 원시 텍스트 전처리\n",
    "text_corpus = \"In this book authored by Sohom Ghosh and Dwight Gunning, we shall learnning how to pracess Natueral Language and extract insights from it. The first four chapter will introduce you to the basics of NLP. Later chapters will describe how to deal with complex NLP prajects. If you want to get early access of it, you should book your order now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'book', 'authored', 'by', 'Sohom', 'Ghosh', 'and', 'Dwight', 'Gunning', ',', 'we', 'shall', 'learnning', 'how', 'to', 'pracess', 'Natueral', 'Language', 'and', 'extract', 'insights', 'from', 'it', '.', 'The', 'first', 'four', 'chapter', 'will', 'introduce', 'you', 'to', 'the', 'basics', 'of', 'NLP', '.', 'Later', 'chapters', 'will', 'describe', 'how', 'to', 'deal', 'with', 'complex', 'NLP', 'prajects', '.', 'If', 'you', 'want', 'to', 'get', 'early', 'access', 'of', 'it', ',', 'you', 'should', 'book', 'your', 'order', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text_corpus)\n",
    "#print([tokens[i] for i in range(20)])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this book authored by Show Ghost and Right Running , we shall learning how to process Natural Language and extract insights from it . The first four chapter will introduce you to the basics of LP . Later chapters will describe how to deal with complex LP projects . If you want to get early access of it , you should book your order now .\n",
      "['In', 'this', 'book', 'authored', 'by', 'Show', 'Ghost', 'and', 'Right', 'Running', ',', 'we', 'shall', 'learning', 'how', 'to', 'process', 'Natural', 'Language', 'and']\n"
     ]
    }
   ],
   "source": [
    "sentence_corrected = ' '.join([spell(token) for token in tokens])\n",
    "print(sentence_corrected)\n",
    "tokens_corrected = word_tokenize(sentence_corrected)\n",
    "print([tokens_corrected[i] for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'PRP'), ('n', 'VBP')], [('t', 'NN'), ('h', 'NN'), ('i', 'NN'), ('s', 'VBP')], [('b', 'NN'), ('o', 'NN'), ('o', 'NN'), ('k', 'NN')], [('a', 'DT'), ('u', 'JJ'), ('t', 'NN'), ('h', 'NN'), ('o', 'JJ'), ('r', 'NN'), ('e', 'NN'), ('d', 'NN')], [('b', 'NN'), ('y', 'NN')], [('S', 'NNP'), ('h', 'NN'), ('o', 'NN'), ('w', 'NN')], [('G', 'NNP'), ('h', 'NN'), ('o', 'NN'), ('s', 'NN'), ('t', 'NN')], [('a', 'DT'), ('n', 'JJ'), ('d', 'NN')], [('R', 'NN'), ('i', 'NN'), ('g', 'VBP'), ('h', 'NN'), ('t', 'NN')], [('R', 'NNP'), ('u', 'JJ'), ('n', 'JJ'), ('n', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN')], [(',', ',')], [('w', 'NN'), ('e', 'NN')], [('s', 'NN'), ('h', 'VBZ'), ('a', 'DT'), ('l', 'NN'), ('l', 'NN')], [('l', 'NN'), ('e', 'VBZ'), ('a', 'DT'), ('r', 'NN'), ('n', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN')], [('h', 'NN'), ('o', 'NN'), ('w', 'NN')], [('t', 'NN'), ('o', 'NN')], [('p', 'NN'), ('r', 'NN'), ('o', 'IN'), ('c', 'JJ'), ('e', 'NN'), ('s', 'NNS'), ('s', 'VBP')], [('N', 'NNP'), ('a', 'DT'), ('t', 'NN'), ('u', 'JJ'), ('r', 'NN'), ('a', 'DT'), ('l', 'NN')], [('L', 'VB'), ('a', 'DT'), ('n', 'JJ'), ('g', 'NN'), ('u', 'IN'), ('a', 'DT'), ('g', 'NN'), ('e', 'NN')], [('a', 'DT'), ('n', 'JJ'), ('d', 'NN')], [('e', 'NN'), ('x', 'NNP'), ('t', 'NN'), ('r', 'VBZ'), ('a', 'DT'), ('c', 'NN'), ('t', 'NN')], [('i', 'NN'), ('n', 'VBP'), ('s', 'NN'), ('i', 'NN'), ('g', 'VBP'), ('h', 'NN'), ('t', 'NN'), ('s', 'NN')], [('f', 'JJ'), ('r', 'NN'), ('o', 'NN'), ('m', 'NN')], [('i', 'NN'), ('t', 'NN')], [('.', '.')], [('T', 'NNP'), ('h', 'NN'), ('e', 'NN')], [('f', 'NN'), ('i', 'NN'), ('r', 'VBP'), ('s', 'NN'), ('t', 'NN')], [('f', 'JJ'), ('o', 'NN'), ('u', 'JJ'), ('r', 'NN')], [('c', 'NNS'), ('h', 'VBP'), ('a', 'DT'), ('p', 'NN'), ('t', 'NN'), ('e', 'NN'), ('r', 'NN')], [('w', 'NN'), ('i', 'NN'), ('l', 'VBP'), ('l', 'NN')], [('i', 'NN'), ('n', 'VBP'), ('t', 'NN'), ('r', 'NN'), ('o', 'NN'), ('d', 'NN'), ('u', 'JJ'), ('c', 'NN'), ('e', 'NN')], [('y', 'NN'), ('o', 'NN'), ('u', 'NN')], [('t', 'NN'), ('o', 'NN')], [('t', 'NN'), ('h', 'NN'), ('e', 'NN')], [('b', 'VB'), ('a', 'DT'), ('s', 'NN'), ('i', 'NN'), ('c', 'VBP'), ('s', 'NN')], [('o', 'NN'), ('f', 'NN')], [('L', 'NNP'), ('P', 'NNP')], [('.', '.')], [('L', 'VB'), ('a', 'DT'), ('t', 'NN'), ('e', 'NN'), ('r', 'NN')], [('c', 'NNS'), ('h', 'VBP'), ('a', 'DT'), ('p', 'NN'), ('t', 'NN'), ('e', 'NN'), ('r', 'NN'), ('s', 'NN')], [('w', 'NN'), ('i', 'NN'), ('l', 'VBP'), ('l', 'NN')], [('d', 'NN'), ('e', 'NN'), ('s', 'NN'), ('c', 'VBP'), ('r', 'NN'), ('i', 'NN'), ('b', 'VBP'), ('e', 'NN')], [('h', 'NN'), ('o', 'NN'), ('w', 'NN')], [('t', 'NN'), ('o', 'NN')], [('d', 'NN'), ('e', 'VBZ'), ('a', 'DT'), ('l', 'NN')], [('w', 'NN'), ('i', 'NN'), ('t', 'VBP'), ('h', 'NN')], [('c', 'NNS'), ('o', 'VBP'), ('m', 'JJ'), ('p', 'NN'), ('l', 'NN'), ('e', 'NN'), ('x', 'NN')], [('L', 'NNP'), ('P', 'NNP')], [('p', 'NN'), ('r', 'NN'), ('o', 'IN'), ('j', 'NN'), ('e', 'NN'), ('c', 'VBP'), ('t', 'NN'), ('s', 'NN')], [('.', '.')], [('I', 'PRP'), ('f', 'VBP')], [('y', 'NN'), ('o', 'NN'), ('u', 'NN')], [('w', 'VB'), ('a', 'DT'), ('n', 'JJ'), ('t', 'NN')], [('t', 'NN'), ('o', 'NN')], [('g', 'NN'), ('e', 'NN'), ('t', 'NN')], [('e', 'VB'), ('a', 'DT'), ('r', 'NN'), ('l', 'NN'), ('y', 'NN')], [('a', 'DT'), ('c', 'JJ'), ('c', 'NN'), ('e', 'NN'), ('s', 'NN'), ('s', 'NN')], [('o', 'NN'), ('f', 'NN')], [('i', 'NN'), ('t', 'NN')], [(',', ',')], [('y', 'NN'), ('o', 'NN'), ('u', 'NN')], [('s', 'NN'), ('h', 'NN'), ('o', 'JJ'), ('u', 'JJ'), ('l', 'NN'), ('d', 'NN')], [('b', 'NN'), ('o', 'NN'), ('o', 'NN'), ('k', 'NN')], [('y', 'NN'), ('o', 'NN'), ('u', 'JJ'), ('r', 'NN')], [('o', 'JJ'), ('r', 'NN'), ('d', 'NN'), ('e', 'NN'), ('r', 'NN')], [('n', 'JJ'), ('o', 'NN'), ('w', 'NN')], [('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "print([nltk.pos_tag(token) for token in tokens_corrected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'book', 'authored', 'Show', 'Ghost', 'Right', 'Running', ',', 'shall', 'learning', 'process', 'Natural', 'Language', 'extract', 'insights', '.', 'The', 'first', 'four', 'chapter']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "tokens_no_stopwords = [token for token in tokens_corrected if token not in stop_words]\n",
    "print([tokens_no_stopwords[i] for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'book', 'author', 'show', 'ghost', 'right', 'run', ',', 'shall', 'learn', 'process', 'natur', 'languag', 'extract', 'insight', '.', 'the', 'first', 'four', 'chapter']\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer.stem(\"production\")\n",
    "print([stemmer.stem(tokens_no_stopwords[i]) for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In this book authored by Sohom Ghosh and Dwight Gunning, we shall learnning how to pracess Natueral Language and extract insights from it.', 'The first four chapter will introduce you to the basics of NLP.', 'Later chapters will describe how to deal with complex NLP prajects.', 'If you want to get early access of it, you should book your order now.']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "sentence_list = sent_tokenize(text_corpus)\n",
    "print(sentence_list)\n",
    "print(len(sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
