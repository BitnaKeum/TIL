{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 장치를 사용합니다: cpu\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE=100\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print('다음 장치를 사용합니다:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.FashionMNIST('./.data',\n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5,))\n",
    "                                ]))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = trainset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "G = nn.Sequential(    # Sequential : 신경망을 이루는 각 layer에서 수행할 연산들을 입력받아 순서대로 실행\n",
    "        nn.Linear(64, 256),   # 정규분포로부터 뽑은 64차원의 무작위 tensor\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 784),\n",
    "        nn.Tanh()     # Tanh 함수를 통해 -1~1의 값을 가짐\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "D = nn.Sequential(\n",
    "        nn.Linear(784, 256),    # 이미지의 크기는 28*28=784\n",
    "        nn.LeakyReLU(0.2),      # ReLU가 아니라 LeakyReLU를 사용하는 이유 : 양의 기울기만 갖지 않고 약간의 음의 기울기도 갖기 때문에 생성자에 더 강하게 전달됨 (생성자가 학습 시 판별자로부터 기울기를 효과적으로 전달받는 것이 중요)\n",
    "        nn.Linear(256, 256),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(256, 1),\n",
    "        nn.Sigmoid()      # Sigmoid 함수를 통해 0~1의 값을 가짐 (0=가짜, 1=진짜)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = D.to(DEVICE)\n",
    "G = G.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차 함수 : 이진 교차 엔트로피 \n",
    "criterion = nn.BCELoss()    \n",
    "\n",
    "# 최적화 알고리즘 : Adam\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [0/500] d_loss:0.0415 g_loss:4.5327 D(x):0.98 D(G(z)):0.02\n",
      "epoch [1/500] d_loss:0.0149 g_loss:6.3717 D(x):0.99 D(G(z)):0.01\n",
      "epoch [2/500] d_loss:0.0899 g_loss:7.1315 D(x):0.96 D(G(z)):0.00\n",
      "epoch [3/500] d_loss:0.0649 g_loss:6.2821 D(x):0.97 D(G(z)):0.02\n",
      "epoch [4/500] d_loss:0.0677 g_loss:4.9087 D(x):0.98 D(G(z)):0.04\n",
      "epoch [5/500] d_loss:0.1075 g_loss:5.3211 D(x):0.96 D(G(z)):0.02\n",
      "epoch [6/500] d_loss:0.0934 g_loss:6.1173 D(x):0.96 D(G(z)):0.02\n",
      "epoch [7/500] d_loss:0.2204 g_loss:4.1025 D(x):0.93 D(G(z)):0.06\n",
      "epoch [8/500] d_loss:0.1173 g_loss:6.9752 D(x):0.96 D(G(z)):0.04\n",
      "epoch [9/500] d_loss:0.1810 g_loss:4.8100 D(x):0.95 D(G(z)):0.05\n",
      "epoch [10/500] d_loss:0.1656 g_loss:4.7332 D(x):0.93 D(G(z)):0.06\n",
      "epoch [11/500] d_loss:0.3498 g_loss:5.4743 D(x):0.89 D(G(z)):0.03\n",
      "epoch [12/500] d_loss:0.4986 g_loss:3.6416 D(x):0.90 D(G(z)):0.13\n",
      "epoch [13/500] d_loss:0.2413 g_loss:3.7221 D(x):0.95 D(G(z)):0.09\n",
      "epoch [14/500] d_loss:0.2135 g_loss:4.7553 D(x):0.93 D(G(z)):0.05\n",
      "epoch [15/500] d_loss:0.3649 g_loss:3.8424 D(x):0.92 D(G(z)):0.16\n",
      "epoch [16/500] d_loss:0.3118 g_loss:3.3585 D(x):0.92 D(G(z)):0.12\n",
      "epoch [17/500] d_loss:0.4853 g_loss:2.7992 D(x):0.90 D(G(z)):0.21\n",
      "epoch [18/500] d_loss:0.5166 g_loss:3.3457 D(x):0.81 D(G(z)):0.07\n",
      "epoch [19/500] d_loss:0.5397 g_loss:3.2036 D(x):0.85 D(G(z)):0.10\n",
      "epoch [20/500] d_loss:0.3583 g_loss:3.8173 D(x):0.89 D(G(z)):0.09\n",
      "epoch [21/500] d_loss:0.4646 g_loss:3.2426 D(x):0.83 D(G(z)):0.11\n",
      "epoch [22/500] d_loss:0.6264 g_loss:3.7585 D(x):0.81 D(G(z)):0.11\n",
      "epoch [23/500] d_loss:0.3275 g_loss:4.0017 D(x):0.90 D(G(z)):0.09\n",
      "epoch [24/500] d_loss:0.4349 g_loss:2.7870 D(x):0.86 D(G(z)):0.14\n",
      "epoch [25/500] d_loss:0.6675 g_loss:2.8417 D(x):0.90 D(G(z)):0.30\n",
      "epoch [26/500] d_loss:0.6774 g_loss:3.1890 D(x):0.82 D(G(z)):0.13\n",
      "epoch [27/500] d_loss:0.5101 g_loss:2.6363 D(x):0.88 D(G(z)):0.13\n",
      "epoch [28/500] d_loss:0.5754 g_loss:3.4088 D(x):0.85 D(G(z)):0.17\n",
      "epoch [29/500] d_loss:0.5131 g_loss:3.5268 D(x):0.80 D(G(z)):0.11\n",
      "epoch [30/500] d_loss:0.4229 g_loss:3.3125 D(x):0.87 D(G(z)):0.12\n",
      "epoch [31/500] d_loss:0.7283 g_loss:3.1945 D(x):0.70 D(G(z)):0.07\n",
      "epoch [32/500] d_loss:0.5275 g_loss:2.7822 D(x):0.81 D(G(z)):0.08\n",
      "epoch [33/500] d_loss:0.4499 g_loss:3.4439 D(x):0.85 D(G(z)):0.12\n",
      "epoch [34/500] d_loss:0.4829 g_loss:2.7865 D(x):0.87 D(G(z)):0.14\n",
      "epoch [35/500] d_loss:0.5584 g_loss:2.6263 D(x):0.86 D(G(z)):0.20\n",
      "epoch [36/500] d_loss:0.4567 g_loss:3.2202 D(x):0.83 D(G(z)):0.09\n",
      "epoch [37/500] d_loss:0.5066 g_loss:2.5662 D(x):0.88 D(G(z)):0.20\n",
      "epoch [38/500] d_loss:0.4346 g_loss:2.9383 D(x):0.89 D(G(z)):0.19\n",
      "epoch [39/500] d_loss:0.6392 g_loss:2.9651 D(x):0.83 D(G(z)):0.18\n",
      "epoch [40/500] d_loss:0.5947 g_loss:2.4849 D(x):0.77 D(G(z)):0.09\n",
      "epoch [41/500] d_loss:0.6922 g_loss:3.1777 D(x):0.80 D(G(z)):0.15\n",
      "epoch [42/500] d_loss:0.8992 g_loss:2.9009 D(x):0.75 D(G(z)):0.14\n",
      "epoch [43/500] d_loss:0.5186 g_loss:2.9611 D(x):0.80 D(G(z)):0.13\n",
      "epoch [44/500] d_loss:0.6024 g_loss:2.3126 D(x):0.85 D(G(z)):0.24\n",
      "epoch [45/500] d_loss:0.5189 g_loss:2.8091 D(x):0.83 D(G(z)):0.14\n",
      "epoch [46/500] d_loss:0.6180 g_loss:2.5435 D(x):0.80 D(G(z)):0.18\n",
      "epoch [47/500] d_loss:0.6616 g_loss:2.2846 D(x):0.85 D(G(z)):0.26\n",
      "epoch [48/500] d_loss:0.5754 g_loss:3.1470 D(x):0.79 D(G(z)):0.14\n",
      "epoch [49/500] d_loss:0.6339 g_loss:2.4431 D(x):0.81 D(G(z)):0.20\n",
      "epoch [50/500] d_loss:0.4498 g_loss:2.8987 D(x):0.85 D(G(z)):0.17\n",
      "epoch [51/500] d_loss:0.6847 g_loss:2.2373 D(x):0.78 D(G(z)):0.21\n",
      "epoch [52/500] d_loss:0.7725 g_loss:2.6003 D(x):0.72 D(G(z)):0.14\n",
      "epoch [53/500] d_loss:0.7830 g_loss:1.9931 D(x):0.76 D(G(z)):0.24\n",
      "epoch [54/500] d_loss:0.6610 g_loss:2.0395 D(x):0.75 D(G(z)):0.20\n",
      "epoch [55/500] d_loss:0.5159 g_loss:2.3585 D(x):0.82 D(G(z)):0.18\n",
      "epoch [56/500] d_loss:0.7518 g_loss:2.2349 D(x):0.73 D(G(z)):0.18\n",
      "epoch [57/500] d_loss:0.8731 g_loss:2.0743 D(x):0.81 D(G(z)):0.28\n",
      "epoch [58/500] d_loss:0.8807 g_loss:2.4250 D(x):0.75 D(G(z)):0.25\n",
      "epoch [59/500] d_loss:0.7460 g_loss:2.3130 D(x):0.76 D(G(z)):0.19\n",
      "epoch [60/500] d_loss:0.6022 g_loss:2.9288 D(x):0.87 D(G(z)):0.24\n",
      "epoch [61/500] d_loss:1.1856 g_loss:2.0298 D(x):0.83 D(G(z)):0.40\n",
      "epoch [62/500] d_loss:0.7594 g_loss:1.9143 D(x):0.79 D(G(z)):0.27\n",
      "epoch [63/500] d_loss:0.6047 g_loss:2.7116 D(x):0.80 D(G(z)):0.17\n",
      "epoch [64/500] d_loss:0.6204 g_loss:2.6312 D(x):0.76 D(G(z)):0.15\n",
      "epoch [65/500] d_loss:0.6048 g_loss:2.2108 D(x):0.85 D(G(z)):0.26\n",
      "epoch [66/500] d_loss:0.6635 g_loss:1.8923 D(x):0.85 D(G(z)):0.30\n",
      "epoch [67/500] d_loss:0.9089 g_loss:2.7780 D(x):0.73 D(G(z)):0.22\n",
      "epoch [68/500] d_loss:0.5573 g_loss:2.0882 D(x):0.85 D(G(z)):0.25\n",
      "epoch [69/500] d_loss:0.8577 g_loss:1.6071 D(x):0.84 D(G(z)):0.32\n",
      "epoch [70/500] d_loss:0.6784 g_loss:2.2814 D(x):0.76 D(G(z)):0.19\n",
      "epoch [71/500] d_loss:0.6377 g_loss:2.1104 D(x):0.87 D(G(z)):0.30\n",
      "epoch [72/500] d_loss:0.9060 g_loss:2.0249 D(x):0.74 D(G(z)):0.28\n",
      "epoch [73/500] d_loss:0.8289 g_loss:2.3619 D(x):0.71 D(G(z)):0.18\n",
      "epoch [74/500] d_loss:0.6963 g_loss:2.6805 D(x):0.77 D(G(z)):0.19\n",
      "epoch [75/500] d_loss:0.8267 g_loss:1.8999 D(x):0.77 D(G(z)):0.28\n",
      "epoch [76/500] d_loss:0.7723 g_loss:2.5048 D(x):0.74 D(G(z)):0.23\n",
      "epoch [77/500] d_loss:0.9667 g_loss:1.9163 D(x):0.66 D(G(z)):0.25\n",
      "epoch [78/500] d_loss:0.7267 g_loss:2.2941 D(x):0.74 D(G(z)):0.23\n",
      "epoch [79/500] d_loss:0.7107 g_loss:1.9075 D(x):0.77 D(G(z)):0.23\n",
      "epoch [80/500] d_loss:0.8598 g_loss:1.4915 D(x):0.76 D(G(z)):0.32\n",
      "epoch [81/500] d_loss:0.8711 g_loss:1.9017 D(x):0.72 D(G(z)):0.26\n",
      "epoch [82/500] d_loss:0.6713 g_loss:2.0283 D(x):0.85 D(G(z)):0.29\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(EPOCHS):  # GAN 학습\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = images.reshape(BATCH_SIZE, -1).to(DEVICE)\n",
    "        \n",
    "        real_labels = torch.ones(BATCH_SIZE, 1).to(DEVICE)\n",
    "        fake_labels = torch.zeros(BATCH_SIZE, 1).to(DEVICE)\n",
    "        \n",
    "        # discriminator가 진짜 이미지를 진짜로 인식하는 오차 계산\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "\n",
    "        # generator는 무작위 텐서로 가짜 이미지 생성\n",
    "        z = torch.randn(BATCH_SIZE, 64).to(DEVICE)   # 무작위 텐서\n",
    "        fake_images = G(z)\n",
    "\n",
    "        # discriminator가 가짜 이미지를 가짜로 인식하는 오차 계산\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "\n",
    "        # discriminator의 오차 구함\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # discriminator 신경망 학습\n",
    "        d_optimizer.zero_grad()   # 기울기 초기화\n",
    "        g_optimizer.zero_grad()\n",
    "        d_loss.backward()   # 역전파 알고리즘\n",
    "        d_optimizer.step()\n",
    "\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)  # generator가 discriminator를 속였는지에 대한 오차 계산\n",
    "\n",
    "        # generator 학습 (가짜를 진짜로 인식하도록)\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        \n",
    "    # 학습 진행 알아보기\n",
    "    # D(x): 진짜를 진짜로 인식, D(G(z)): 가짜를 진짜로 인식\n",
    "    print('epoch [{}/{}] d_loss:{:.4f} g_loss:{:.4f} D(x):{:.2f} D(G(z)):{:.2f}'.format(epoch, EPOCHS, d_loss.item(), g_loss.item(), real_score.mean().item(), fake_score.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator가 만들어낸 가짜 이미지 시각화\n",
    "z = torch.randn(BATCH_SIZE, 64).to(DEVICE)\n",
    "fake_images = G(z)\n",
    "\n",
    "for i in range(10):\n",
    "    fake_images_img = np.reshape(fake_images.data.cpu().numpy()[i], (28,28))\n",
    "    plt.imshow(fake_images_img, cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
